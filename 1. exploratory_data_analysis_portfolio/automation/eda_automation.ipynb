{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855c5987",
   "metadata": {},
   "source": [
    "### EDA Automation Notebook\n",
    "This notebook is a jupyter notebook template for all my EDA projects. This will contain various checks that I commonly use in my projects. Since this is an automation or a template, this does not contain any test data. It will contain the following checks as seen below:\n",
    "- Step 1: Loading the data\n",
    "- Step 2: Checking the basic data information\n",
    "- Step 3: Data quality assessment\n",
    "- Step 4: Summary statistics\n",
    "- Step 5: Correlations and outliers\n",
    "- Step 6: Answering questions\n",
    "\n",
    "There will be another python file and this prints out the pdf report instead of the notebook as this is used for data exploration only and not for answering questions. I used ChatGPT to help with the documentation of the code here as well as to identify the better and more efficient code to implement.\n",
    "\n",
    "Will fix code so that all items will output on the txt/pdf file. For now, outputs are in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf5e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "from IPython.display import display\n",
    "import platform\n",
    "import subprocess\n",
    "import datetime\n",
    "import textdistance\n",
    "from PyPDF2 import PdfMerger\n",
    "from fpdf import FPDF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a08d8",
   "metadata": {},
   "source": [
    "##### Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f597ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sheet_name=None, encoding=\"utf-8\"):\n",
    "    if len(file_path) == 0:\n",
    "        raise FileNotFoundError(\"File path cannot be blank.\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    ext_type = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "    try:\n",
    "        if ext_type == \".csv\":\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UTF-8 failed, trying ISO-8859-1...\")\n",
    "                df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "        elif ext_type in [\".xls\", \".xlsx\"]:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name if sheet_name else 0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only CSV and Excel files are supported.\")\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Error reading file: {exc}\")\n",
    "\n",
    "    print(f\"Data loaded successfully from {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e42e2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 failed, trying ISO-8859-1...\n",
      "✅ Data loaded successfully from data.csv\n"
     ]
    }
   ],
   "source": [
    "#Interactive data loading for targeted file path loading. This allows to choose any file at any location.\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = input(\"Enter file path (e.g., data.csv or C:/folder/data.csv): \").strip()\n",
    "    sheet_name_input = input(\"Optional: Enter Excel sheet name (press ENTER if not applicable): \").strip()\n",
    "    sheet_name = sheet_name_input if sheet_name_input else None\n",
    "\n",
    "    try:\n",
    "        df = load_data(file_path, sheet_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b765872",
   "metadata": {},
   "source": [
    "##### Step 2: Checking the basic data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b6bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_information(df, preview_rows=5, file_path=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a PDF summary of basic data information, including:\n",
    "    - Shape and data types\n",
    "    - Missing values\n",
    "    - Descriptive statistics\n",
    "    - Unique values per column\n",
    "    - Data preview\n",
    "\n",
    "    Saves the output as 'eda_automation_step2.pdf'.\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['number']).shape[1]\n",
    "    non_numeric_cols = df.select_dtypes(exclude=['number']).shape[1]\n",
    "\n",
    "    # Accumulate all lines\n",
    "    file_lines = []\n",
    "\n",
    "    def out(line):\n",
    "        file_lines.append(str(line))\n",
    "\n",
    "    def out_df(sub_df):\n",
    "        file_lines.append(sub_df.to_string())\n",
    "\n",
    "    out(\"=\" * 110)\n",
    "    out(\"Basic Data Information:\")\n",
    "    out(f\"There are {df.shape[0]:,} rows and {df.shape[1]:,} columns in the data.\")\n",
    "    out(\"Here are the data types per column:\")\n",
    "    out_df(df.dtypes.to_frame('Column data types'))\n",
    "\n",
    "    out(f\"\\nThere are {numeric_cols} numeric columns and {non_numeric_cols} non-numeric columns.\")\n",
    "\n",
    "    out(\"\\n\" + \"-\" * 110 + \"\\nMissing Values:\")\n",
    "    mv = df.isnull().sum()\n",
    "    mv = mv[mv > 0].sort_values(ascending=False)\n",
    "    if not mv.empty:\n",
    "        out_df(mv.to_frame('count_of_missing_values'))\n",
    "    else:\n",
    "        out(\"No missing values in the dataset.\")\n",
    "\n",
    "    out(\"\\n\" + \"-\" * 110 + \"\\nDescriptive Statistics:\")\n",
    "    out_df(df.describe(include='all').T)\n",
    "\n",
    "    out(\"\\n\" + \"-\" * 110 + \"\\nUnique items per column:\")\n",
    "    c_uniques = df.nunique().sort_values(ascending=False)\n",
    "    out_df(c_uniques.to_frame('unique_values'))\n",
    "\n",
    "    if preview_rows:\n",
    "        out(f\"\\nData preview: first {preview_rows} rows\")\n",
    "        out_df(df.head(preview_rows))\n",
    "\n",
    "    out(\"\\n\" + \"=\" * 110)\n",
    "    out(f\"End of basic data summary for {file_path}\")\n",
    "    out(\"=\" * 110)\n",
    "\n",
    "    pdf = FPDF(\"L\", \"mm\", \"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Courier\", size=8)\n",
    "\n",
    "    for line in file_lines:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            pdf.multi_cell(0, 5, subline)\n",
    "\n",
    "    pdf_path = f\"{file_path}_eda_automation_step2.pdf\"\n",
    "    pdf.output(pdf_path)\n",
    "    print(f\"✅ PDF saved as '{pdf_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20fbee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF saved as 'data.csv_eda_automation_step2.pdf'\n"
     ]
    }
   ],
   "source": [
    "basic_data_information(df, preview_rows=5, file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148932a",
   "metadata": {},
   "source": [
    "##### Step 3: Data quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831c343",
   "metadata": {},
   "source": [
    "Data quality assessment offers an deep dive on the quality of the data. While some methods are similar with that on step 2, step 3 will have a deeper quality check to ensure that the data is 'ready' for further analysis.\n",
    "\n",
    "Adding in a functionality that prints the output to a pdf instead of the console to review the outputs for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def data_quality_deep_dive(df, preview_rows=5, size_threshold=300, force_mode=\"container\", file_prefix=\"quality_deep_dive_summary\", file_path=\"data\"):\n",
    "#     mode = force_mode\n",
    "#     output_lines = []\n",
    "\n",
    "#     def add_output(line):\n",
    "#         if mode == \"print\":\n",
    "#             print(line)\n",
    "#         else:\n",
    "#             output_lines.append(str(line))\n",
    "    \n",
    "#     def add_df_output(sub_df, title):\n",
    "#         if mode == \"print\":\n",
    "#             print(\"\\n\" + \"=\" * 100 + f\"\\n{title}: \\n\")\n",
    "#             print(sub_df)\n",
    "#         else:\n",
    "#             output_lines.append(f\"\\n\\n{title}: \\n\")\n",
    "#             output_lines.append(sub_df.to_string())\n",
    "\n",
    "#     add_output(\"=\" * 110)\n",
    "#     add_output(f\"DATA QUALITY ASSESSMENT FOR {file_path} with {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "#     add_output(\"=\" * 110)\n",
    "\n",
    "#     title_items = ['Duplicate Rows', \n",
    "#                 'Mixed Data Types', \n",
    "#                 'Capitalization Inconsistencies',\n",
    "#                 'Unexpected/Unknown Values',\n",
    "#                 'Numeric Outliers or Skew Checks',\n",
    "#                 'Data Preview'\n",
    "#                 ]\n",
    "\n",
    "#     for index, title in enumerate(title_items, 1):\n",
    "#         add_output(\"\\n\" + \"-\" * 110 + \"\\n\" + f\"[{index}] {title}:\" + \"\\n\" + \"-\" * 110 + \"\\n\")\n",
    "        \n",
    "#         if index == 1:  # Duplicate Rows\n",
    "#             duplicate_rows = df[df.duplicated()]\n",
    "#             if not duplicate_rows.empty:\n",
    "#                 add_output(f\"There are {len(duplicate_rows):,} duplicate rows in the dataset.\\n\")\n",
    "#                 duplicate_indices = duplicate_rows.index.tolist()\n",
    "#                 add_output(\"Duplicate row indices (showing up to 100):\")\n",
    "#                 add_output(str(duplicate_indices[:100]))\n",
    "#                 if len(duplicate_rows) <= size_threshold:\n",
    "#                     add_output(duplicate_rows.to_string(index=False))\n",
    "#                 else:\n",
    "#                     add_output(\"Too many duplicates to display; showing top 5:\")\n",
    "#                     add_output(duplicate_rows.head(5).to_string(index=False))\n",
    "#             else:\n",
    "#                 add_output(\"No exact duplicate rows found.\")\n",
    "\n",
    "#         elif index == 2: #Mixed data types\n",
    "#             mixed_data_types_issues = {}\n",
    "\n",
    "#             for col in df.columns:\n",
    "#                 mixed_types = df[col].dropna().map(type).value_counts()\n",
    "#                 if len(mixed_types) > 1:\n",
    "#                     mixed_data_types_issues[col] = mixed_types\n",
    "\n",
    "#             if mixed_data_types_issues:\n",
    "#                 add_output(f\"Found {len(mixed_data_types_issues)} columns with mixed data types:\\n\")\n",
    "#                 for col, types in mixed_data_types_issues.items():\n",
    "#                     add_output(f\"- Column '{col}' has multiple data types:\")\n",
    "#                     for t, count in types.items():\n",
    "#                         add_output(f\"    • {t.__name__}: {count:,} values\")\n",
    "#             else:\n",
    "#                 add_output(\"No mixed data types found in all columns.\")\n",
    "                    \n",
    "#         elif index == 3:  # Capitalization Inconsistencies\n",
    "#             obj_cols = df.select_dtypes(include='object').columns\n",
    "#             cap_issues_found = False\n",
    "#             for col in obj_cols:\n",
    "#                 vals = df[col].dropna().unique()\n",
    "#                 groups = {}\n",
    "#                 for v in vals:\n",
    "#                     key = v.lower()\n",
    "#                     groups.setdefault(key, set()).add(v)\n",
    "#                 for key, variations in groups.items():\n",
    "#                     if len(variations) > 1:\n",
    "#                         cap_issues_found = True\n",
    "#                         add_output(f\"Column '{col}', has variations for '{key}': {variations}\")\n",
    "#             if not cap_issues_found:\n",
    "#                 add_output(\"No capitalization inconsistencies found.\")\n",
    "\n",
    "#         elif index == 4:  # Unexpected/Unknown Values\n",
    "#             known_placeholders = {'n/a', 'na', 'none', 'null', 'unknown', '-', '', 'not applicable'}\n",
    "#             threshold = 0.01\n",
    "            \n",
    "#             rare_or_unknown_found = False\n",
    "            \n",
    "#             for col in df.select_dtypes(include='object'):\n",
    "#                 val_counts = df[col].dropna().value_counts(normalize=True)\n",
    "#                 to_flag = val_counts[val_counts < threshold].index.tolist()\n",
    "                \n",
    "#                 placeholder_hits = [v for v in df[col].dropna().unique()\n",
    "#                                     if str(v).strip().lower() in known_placeholders]\n",
    "                \n",
    "#                 if to_flag or placeholder_hits:\n",
    "#                     rare_or_unknown_found = True\n",
    "#                     add_output(f\"\\nColumn: '{col}'\")\n",
    "#                     if placeholder_hits:\n",
    "#                         add_output(f\" - Found placeholder-like values: {placeholder_hits}\")\n",
    "#                     if to_flag:\n",
    "#                         add_output(f\" - Rare values (<1% occurrence): {to_flag[:5]}{'...' if len(to_flag) > 5 else ''}\")\n",
    "            \n",
    "#             if not rare_or_unknown_found:\n",
    "#                 add_output(\"No values flagged.\")\n",
    "\n",
    "#         elif index == 5:  # Numeric Outliers or Skew Checks\n",
    "#             numeric_cols = df.select_dtypes(include='number').columns\n",
    "#             if len(numeric_cols) == 0:\n",
    "#                 add_output(\"No numeric columns found.\")\n",
    "#             else:\n",
    "#                 for col in numeric_cols:\n",
    "#                     series = df[col].dropna()\n",
    "#                     if len(series) == 0:\n",
    "#                         continue\n",
    "\n",
    "#                     q1 = series.quantile(0.25)\n",
    "#                     q3 = series.quantile(0.75)\n",
    "#                     iqr = q3 - q1\n",
    "#                     lower = q1 - 1.5 * iqr\n",
    "#                     upper = q3 + 1.5 * iqr\n",
    "#                     outliers = series[(series < lower) | (series > upper)]\n",
    "\n",
    "#                     add_output(f\"\\nColumn: {col}\")\n",
    "#                     add_output(f\" - Min: {series.min():,.2f}, Max: {series.max():,.2f}, Mean: {series.mean():,.2f}, Std: {series.std():,.2f}\")\n",
    "#                     add_output(f\" - Outliers (IQR rule): {len(outliers):,}\")\n",
    "\n",
    "#                     if len(outliers) > 0 and len(outliers) <= size_threshold:\n",
    "#                         add_output(f\"Sample outlier values: {outliers.unique()[:5].tolist()}\")\n",
    "#                     elif len(outliers) > size_threshold:\n",
    "#                         add_output(\"Too many outliers to display.\")\n",
    "\n",
    "#         elif index == 6:  # Data Preview\n",
    "#             add_output(f\"Previewing the first {preview_rows} rows of the dataset:\\n\")\n",
    "#             if preview_rows > 0:\n",
    "#                 preview = df.head(preview_rows).to_string(index=False)\n",
    "#                 add_output(preview)\n",
    "#             else:\n",
    "#                 add_output(\"Preview  skipped because preview_rows is set to 0.\")\n",
    "\n",
    "#     return output_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f38958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_lines_cdd = data_quality_deep_dive(df, preview_rows=5, size_threshold=300, force_mode=\"file\", file_path=\"my_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90eb62c",
   "metadata": {},
   "source": [
    "##### Step 4: Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b87904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019192af",
   "metadata": {},
   "source": [
    "##### Step 5: Correlations and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45692b3f",
   "metadata": {},
   "source": [
    "##### Step 6: Creation of report output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_delete_eda_pdfs(output_filename=\"eda_report.pdf\"):\n",
    "    \"\"\"\n",
    "    Merges PDF files named 'eda_automation_step1.pdf' through\n",
    "    'eda_automation_step4.pdf' into a single PDF, and then deletes\n",
    "    the source PDF files.\n",
    "\n",
    "    Args:\n",
    "        output_filename (str): The desired name for the merged PDF file.\n",
    "                               Defaults to \"eda_report.pdf\".\n",
    "    \"\"\"\n",
    "    merger = PdfMerger()\n",
    "    pdf_files = [\n",
    "        \"eda_automation_step1.pdf\",\n",
    "        \"eda_automation_step2.pdf\",\n",
    "        \"eda_automation_step3.pdf\",\n",
    "        \"eda_automation_step4.pdf\"\n",
    "    ]\n",
    "\n",
    "    # List to keep track of files that were actually appended (to delete only those)\n",
    "    appended_files = []\n",
    "\n",
    "    print(\"Attempting to merge the following PDF files:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        if os.path.exists(pdf_file):\n",
    "            print(f\"- {pdf_file}\")\n",
    "            merger.append(pdf_file)\n",
    "            appended_files.append(pdf_file)\n",
    "        else:\n",
    "            print(f\"Warning: '{pdf_file}' not found. Skipping.\")\n",
    "\n",
    "    try:\n",
    "        if appended_files: # Only write if there's content to merge\n",
    "            with open(output_filename, \"wb\") as output_file:\n",
    "                merger.write(output_file)\n",
    "            print(f\"\\nSuccessfully merged PDFs into '{output_filename}'\")\n",
    "\n",
    "            # Delete the source PDF files\n",
    "            print(\"Attempting to delete source PDF files:\")\n",
    "            for pdf_file in appended_files:\n",
    "                try:\n",
    "                    os.remove(pdf_file)\n",
    "                    print(f\"Successfully deleted '{pdf_file}'\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting '{pdf_file}': {e}\")\n",
    "        else:\n",
    "            print(\"No PDF files were found to merge. No output file created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the merging process: {e}\")\n",
    "    finally:\n",
    "        merger.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_and_delete_eda_pdfs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

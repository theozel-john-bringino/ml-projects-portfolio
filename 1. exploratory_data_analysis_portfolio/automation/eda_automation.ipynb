{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855c5987",
   "metadata": {},
   "source": [
    "### EDA Automation Notebook\n",
    "This notebook is a jupyter notebook template for all my EDA projects. This will contain various checks that I commonly use in my projects. Since this is an automation or a template, this does not contain any test data. It will contain the following checks as seen below:\n",
    "- Step 1: Loading the data\n",
    "- Step 2: Checking the basic data information\n",
    "- Step 3: Data quality assessment\n",
    "- Step 4: Summary statistics\n",
    "- Step 5: Correlations and outliers\n",
    "- Step 6: Answering questions\n",
    "\n",
    "There will be another python file and this prints out the pdf report instead of the notebook as this is used for data exploration only and not for answering questions. I used ChatGPT to help with the documentation of the code here as well as to identify the better and more efficient code to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf5e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "from IPython.display import display\n",
    "import platform\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a08d8",
   "metadata": {},
   "source": [
    "##### Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f597ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sheet_name = None, encoding = \"utf-8\"):\n",
    "    \"\"\"\n",
    "    Load a dataset either from a CSV or Excel file from any path provide by the user.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the file, can be absolute or relative.\n",
    "        sheet_name (str or int, optional): Sheet name is used as index for files.\n",
    "    \n",
    "    Output:\n",
    "        dataframe: Data from file is loaded as a dataframe. Loads 50 rows(head) of data for better checking.\n",
    "    \"\"\"\n",
    "    if len(file_path) == 0:\n",
    "        raise FileNotFoundError(f\"File path cannot be blank.\")\n",
    "    #Error handling for files if file cannot be located\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found for {file_path}. Ensure file is inside the folder or input the whole path instead.\")\n",
    "\n",
    "    ext_type = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "    #File type handling for CSV and Excel files as well\n",
    "    try:    \n",
    "        if ext_type == \".csv\":\n",
    "            #Unicode decode error handling\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding = encoding)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UTF-8 failed, trying ISO-8859-1...\")\n",
    "                df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")\n",
    "        \n",
    "        elif ext_type in [\".xls\", \"xlsx\"]:\n",
    "            df = pd.read_excel(file_path, sheet_name = sheet_name if sheet_name else 0)\n",
    "        \n",
    "        #Error raising for unsupported file types\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. This notebook can only load CSV or Excel files.\")\n",
    "\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Error reading file: {exc}\")\n",
    "\n",
    "    print(f\"Data loaded successfully from {file_path}\")\n",
    "    display(df.head(50))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interactive data loading for targeted file path loading. This allows to choose any file at any location.\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = input(\"Enter file path (ex. data.csv or C:/folder/data.csv): \").strip()\n",
    "    sheet_name = input(\"Optional: Specify sheet name or press ENTER to load the file directly.\")\n",
    "    sheet_name = sheet_name if sheet_name else None\n",
    "\n",
    "    df = load_data(file_path, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b765872",
   "metadata": {},
   "source": [
    "##### Step 2: Checking the basic data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12b6bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_information(df, preview_rows = 5):\n",
    "      \"\"\"\n",
    "      This function prints the basic data information of the dataframe including:\n",
    "      - Shape\n",
    "      - Data types\n",
    "      - Missing values\n",
    "      - Descriptive statistics\n",
    "      - Unique counts\n",
    "      - Preview of rows\n",
    "\n",
    "      Parameters: \n",
    "      df (pd.DataFrame): Data to be summarized.\n",
    "      preview_rows(int): How many rows to preview, can set to 0 to skip.\n",
    "      \"\"\"\n",
    "      #Set higher display limits to prevent results from getting cutoff when printing\n",
    "      pd.set_option('display.max_columns', 100)   # set max number of columns to show\n",
    "      pd.set_option('display.max_rows', 100)      # set number of max rows to show\n",
    "      pd.set_option('display.max_colwidth', None) # set to None to prevent cell content from truncating\n",
    "      numeric_cols = df.select_dtypes(include = ['number']).shape[1]\n",
    "      non_numeric_cols = df.select_dtypes(exclude = ['number']).shape[1]\n",
    "\n",
    "      print(\"=\" * 110 + \"\\n\" +\"Basic Data Information: \" + \"\\n\" +\n",
    "            f\"There are {df.shape[0]:,} rows and {df.shape[1]:,} columns in the data.\" + \"\\n\" +\n",
    "            \"Here are the data types per column:\")\n",
    "      display(df.dtypes.to_frame('Column data types'))\n",
    "      print(f\"There are {numeric_cols} numeric columns and {non_numeric_cols} non-numeric columns in the dataset.\")\n",
    "            \n",
    "\n",
    "      print(\"\\n\" + \"-\" * 110 + \"\\n\" * 2 + \"Missing Values: \")\n",
    "      mv = df.isnull().sum()\n",
    "      mv = mv[mv > 0].sort_values(ascending = False)\n",
    "      if not mv.empty:\n",
    "            display(mv.to_frame('count_of_missing_values'))\n",
    "      else:\n",
    "            print(\"No missing values in the dataset.\")\n",
    "\n",
    "      print(\"\\n\" + \"-\" * 110 + \"\\n\" * 2 + \"Descriptive Statistics: \")\n",
    "      display(df.describe(include = 'all').T)\n",
    "\n",
    "      print(\"\\n\" + \"-\" * 110 + \"\\n\" * 2 + \"Unique items per column: \")\n",
    "      c_uniques = df.nunique().sort_values(ascending = False)\n",
    "      display(c_uniques.to_frame('unique_values'))\n",
    "\n",
    "      if preview_rows:\n",
    "            print(f\"Data preview: first {preview_rows} rows\")\n",
    "            display(df.head(preview_rows))\n",
    "\n",
    "      print(\"\\n\" + \"=\" * 110 + \"\\n\" + f\"End of summary for {file_path}.\" \n",
    "            + \"\\n\" + \"=\" * 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_data_information(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148932a",
   "metadata": {},
   "source": [
    "##### Step 3: Data quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831c343",
   "metadata": {},
   "source": [
    "Data quality assessment offers an deep dive on the quality of the data. While some methods are similar with that on step 2, step 3 will have a deeper quality check to ensure that the data is 'ready' for further analysis.\n",
    "\n",
    "Adding in a functionality that prints the output to a pdf instead of the console to review the outputs for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_deep_dive(df, preview_rows=5, size_threshold=300, force_mode=None, file_prefix=\"quality_deep_dive_summary\"):\n",
    "    \"\"\"\n",
    "    Perform a deeper dive in terms of quality assessment on the dataset specifically with:\n",
    "    - Duplicates\n",
    "    - Mixed data types\n",
    "    - Capitalization inconsistencies\n",
    "    - Unexpected values\n",
    "    - Optionally export to a pdf file if the output is too large\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe loaded from steps above.\n",
    "    preview_rows (int): Max number of rows to preview.\n",
    "    size_threshold (int): Max row or columns before triggering file output.\n",
    "    force_mode (str): Allows the function to determine whether to \"print\", \"file\", or None (auto-detect based on size).\n",
    "    file_prefix (str): Name prefix for output file if file mode is used.\n",
    "    \"\"\"\n",
    "\n",
    "    #Identify if output will be printed in the results or exported to a file\n",
    "    mode = force_mode\n",
    "    if mode is None: \n",
    "        if df.shape[0] > size_threshold or df.shape[1] > size_threshold:\n",
    "            mode =\"file\"\n",
    "        else:\n",
    "            mode = \"print\"\n",
    "    output_lines = []\n",
    "\n",
    "    def add_output(line):\n",
    "        if mode == \"print\":\n",
    "            print(line)\n",
    "        else:\n",
    "            output_lines.append(str(line))\n",
    "    \n",
    "    def add_df_output(sub_df, title):\n",
    "        if mode == \"print\":\n",
    "            print(\"\\n\" + \"=\" * 100 + f\"\\n{title}: \\n\")\n",
    "        else:\n",
    "            output_lines.append(f\"\\n\\n{title}: \\n\")\n",
    "            output_lines.append(sub_df.to_string())\n",
    "\n",
    "    add_output(\"=\" * 110)\n",
    "    add_output(f\"DATA QUALITY ASSESSMENT FOR {file_path} with {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "    add_output(\"=\" * 110)\n",
    "\n",
    "    #Placeholder creations\n",
    "    title_items = ['Duplicate Rows', \n",
    "                'Mixed Data Types', \n",
    "                'Capitalization Inconsistencies',\n",
    "                'Unexpected/Unknown Values',\n",
    "                'Numeric Outliers or Skew Checks',\n",
    "                'Data Preview'\n",
    "                ]\n",
    "\n",
    "    for index, title in enumerate(title_items, 1):\n",
    "        add_output(\"\\n\" + \"-\" * 100 + f\"[{index}] {title}:\" + \"\\n\" + \"-\" * 100)\n",
    "        \n",
    "        if index == 1:  # Duplicate Rows\n",
    "            duplicate_rows = df[df.duplicated()]\n",
    "            if not duplicate_rows.empty:\n",
    "                add_output(f\"There are {len(duplicate_rows):,} duplicate rows in the dataset.\\n\")\n",
    "\n",
    "                #Print duplicate row indices - limited for large datasets\n",
    "                duplicate_indices = duplicate_rows.index.tolist()\n",
    "                add_output(\"Duplicate row indices (showing up to 100):\")\n",
    "                add_output(str(duplicate_indices[:100]))  # limit to first 100 only\n",
    "\n",
    "                if len(duplicate_rows) <= size_threshold:\n",
    "                    add_output(duplicate_rows.to_string(index=False))\n",
    "                else:\n",
    "                    add_output(\"Too many duplicates to display; showing top 5:\")\n",
    "                    add_output(duplicate_rows.head(5).to_string(index=False))\n",
    "            else:\n",
    "                add_output(\"No exact duplicate rows found.\")\n",
    "\n",
    "        elif index == 2: #Mixed data types\n",
    "            mixed_data_types_issues = {}\n",
    "\n",
    "            for col in df.columns:\n",
    "                mixed_types = df[col].dropna().map(type).value_counts()\n",
    "\n",
    "                if len(mixed_types) > 1:\n",
    "                    mixed_data_types_issues[col] = mixed_types\n",
    "\n",
    "            if mixed_data_types_issues:\n",
    "                add_output(f\"Found {len(mixed_data_types_issues)} columns with mixed data types:\\n\")\n",
    "                for col, types in mixed_data_types_issues.items():\n",
    "                    add_output(f\"- Column '{col}' has multiple data types:\")\n",
    "                    for t, count in types.items():\n",
    "                        add_output(f\"    • {t.__name__}: {count:,} values\")\n",
    "            else:\n",
    "                add_output(\"No mixed data types found in all columns.\")\n",
    "            \n",
    "        elif index == 3: #Capitalization inconsistencies \n",
    "            return None\n",
    "        elif index == 4: #Unexpected/unknown values \n",
    "            return None\n",
    "        elif index == 5: #Numeric outliers or skew checks\n",
    "            return None\n",
    "        elif index == 6: #Data preview\n",
    "            return None\n",
    "        \n",
    "        # Final output\n",
    "    if mode == \"file\":\n",
    "        output_file = f\"{file_prefix}.txt\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(output_lines))\n",
    "        print(f\"\\nData quality report written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90eb62c",
   "metadata": {},
   "source": [
    "##### Step 4: Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019192af",
   "metadata": {},
   "source": [
    "##### Step 5: Correlations and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45692b3f",
   "metadata": {},
   "source": [
    "##### Step 6: Answering questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
